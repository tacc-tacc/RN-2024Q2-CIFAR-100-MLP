{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"tpu1vmV38","dataSources":[{"sourceId":84511,"databundleVersionId":9468663,"sourceType":"competition"}],"dockerImageVersionId":30788,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"TP1: Implementación de una red MLP para clasificación de imágenes de CIFAR 100\n\nHecho por: Tomás Castro\n\nPASO 1: Abro todos los dataset de entrenamiento y prueba (ya se tantearon los datos brevemente en otra ocasión, así que en este cuaderno voy a ir directamente al entrenamiento)","metadata":{}},{"cell_type":"code","source":"# Cargo todos los datasets\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pickle\n%load_ext tensorboard\n\nINPUT_DIR = \"/kaggle/input/dl-itba-cifar-100-2024-q-1/\"\n\nx_train_ = np.load(INPUT_DIR+\"x_train.npy\")/255.\nx_test = np.load(INPUT_DIR+\"x_test.npy\")/255.\ny_train_coarse_ = np.load(INPUT_DIR+\"y_train_coarse.npy\")\ny_train_fine_ = np.load(INPUT_DIR+\"y_train_fine.npy\")\nwith open(INPUT_DIR+\"fine_label_names.pck\", \"rb\") as f:\n    labels_fine = pickle.load(f)\nwith open(INPUT_DIR+\"coarse_label_names.pck\", \"rb\") as f:\n    labels_coarse = pickle.load(f)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-11-08T14:58:40.317431Z","iopub.execute_input":"2024-11-08T14:58:40.318278Z","iopub.status.idle":"2024-11-08T14:58:43.21252Z","shell.execute_reply.started":"2024-11-08T14:58:40.318222Z","shell.execute_reply":"2024-11-08T14:58:43.211565Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"PASO 2: Activo los TPU","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\n# Detect TPU, return appropriate distribution strategy\ntry:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver() \n    print('Running on TPU ', tpu.master())\nexcept ValueError:\n    tpu = None\n\nif tpu:\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nelse:\n    strategy = tf.distribute.get_strategy() \n\nprint(\"REPLICAS: \", strategy.num_replicas_in_sync)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"\nPASO 3: Importo todas las funciones que voy a necesitar de keras y sklearn, además de que hago el augmentation sobre los datos de entrada.\n\nTodos los callbacks fueron muy útiles. En concreto el reduceLRonPlateau ayudaba a afinar la optimización cuando el modelo se estancaba en un mismo punto, y el EarlyStopping ayudó a prevenir el overfitting, detectando cuando ya era imposible mejorar la precisión de validación. Se redujo la \"paciencia\" de 10 épocas a 5 épocas ya que se verificó que nunca hubo nuevo progreso cuando la optimización se estancaba en más de 5 etapas consecutivas. Debido al gran tamaño de las capas que se decidió adoptar (ver siguiente sección), se aumentó el batch size a 128.\n\nNOTA: Como las imágenes son de baja resolución, no puedo variar demasiado el contenido original de las imágenes, de lo contrario la imagen pierde demasiada claridad (que incluso viéndola visualmente es muy baja y difícil en algunos casos determinar de qué se trata), lo que confundiría más a mi modelo.","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Input, Flatten, Dense, BatchNormalization, Activation, Dropout\nfrom tensorflow.keras.optimizers import SGD, Adam, RMSprop\nfrom tensorflow.keras.regularizers import l1, l2\nfrom tensorflow.keras.constraints import MaxNorm\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint, TensorBoard\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom sklearn.model_selection import train_test_split\n\n# Separación de datos en entrenamiento y validación\nx_train, x_val, y_train, y_val = train_test_split(x_train_, y_train_fine_, test_size=0.2, stratify=y_train_fine_)\n\n#Augmentation\ngen = ImageDataGenerator(\n        width_shift_range=0.05,\n        height_shift_range=0.05,\n        shear_range=0.05,\n        zoom_range=0.05,    \n        rotation_range = 15,\n        horizontal_flip=True)\nflow = gen.flow(x_train, y_train, batch_size=128)\n\n# Definición de callbacks\nrlrop = ReduceLROnPlateau(\n    monitor = \"val_accuracy\",\n    factor = 0.5,\n    patience = 3,\n    verbose = 1,\n    min_lr = 1e-5\n)\nmc = ModelCheckpoint(\n    \"best.weights.h5\",\n    monitor = \"val_accuracy\",\n    verbose = 1,\n    save_best_only = True,\n    save_weights_only = True,\n)\nes = EarlyStopping(\n    monitor = \"val_accuracy\",\n    patience = 5, \n    verbose = 1,\n    restore_best_weights = True,\n)\ntb = TensorBoard(\n    log_dir=\"logs\",\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-08T05:32:01.280137Z","iopub.execute_input":"2024-11-08T05:32:01.280672Z","iopub.status.idle":"2024-11-08T05:32:02.076081Z","shell.execute_reply.started":"2024-11-08T05:32:01.280619Z","shell.execute_reply":"2024-11-08T05:32:02.074401Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"PASO 4: Defino mi arquitectura. Tras diversas pruebas, se establecieron cuatro capas densas, iniciando con la primera capa del tamaño igual a la de entrada y luego se va reduciendo de a 1024 neuronas (32*32) hasta llegar a 100 en la última que es el tamaño de la salida. Se intercalaron capas de Batch Normalization antes de poner la activación, con muy buenos resultados y acelerando notoriamente el entrenamiento. Inicialmente la precisión de entrenamiento era mucho más grande que la de validación lo que era un indicio de sobreadaptación, pero tras haber agregado el augmentation las precisiones se igualaron. \n\nOtras cosas que se intentaron y no funcionaron:\n- Agregar regularizadores L1 y L2 en las capas densas: el modelo ajustaba peor. Comenzando con el parámetro de regularización en 0.001, la precisión en CV quedaba muy por debajo de la precisión en el entrenamiento. Posteriormente se fue disminuyendo hasta 0,000000000001. Si bien la diferencia entre ambos valores se mitigaba bastante y el modelo convergía bastante más rápido (alrededor de 15 épocas menos), la precisión en CV nunca llegó a superar 0,35, quedando siempre debajo del 0,37 que se obtuvo sin el regularizador.\n- Restringir la norma de los pesos con max_norm. Lo único que se logró fue hacer más lento el proceso de convergencia y nunca superó el récord obtenido en CV.\n- Agregar capas de Dropout luego de las de detección relu: se supone que debería reducir la brecha entre las precisiones de entrenamiento y validación, y si bien esto efectivamente se cumplió, el modelo tardó más en converger y la precisión máxima obtenida fue mucho menor (no superó 0,32 en CV).\n- Agregar más capas densas al comienzo, de mayor tamaño: el entrenamiento era siempre más lento, tradaba más en converger y la precisión de CV quedaba considerablemente debajo de la de entrenamiento por overfitting.\n- Modificar la tasa de aprendizaje: siempre se llegó a resultados finales peores, lo que indica que 0,001 es o está muy cerca de su valor óptimo. Tampoco mejoró la convergencia cambiar el tipo de optimizador a SGD o RMSprop.","metadata":{}},{"cell_type":"code","source":"with strategy.scope():\n    model = Sequential()\n    model.add(Input(shape=(32,32,3)))\n    model.add(Flatten())\n    model.add(Dense(32*32*3, use_bias=False))\n    model.add(BatchNormalization())\n    model.add(Activation('relu'))\n    model.add(Dense(32*32*2, use_bias=False))\n    model.add(BatchNormalization())\n    model.add(Activation('relu'))\n    model.add(Dense(32*32, use_bias=False))\n    model.add(BatchNormalization())\n    model.add(Activation('relu'))\n    model.add(Dense(100, use_bias=False))\n    model.add(BatchNormalization())\n    model.add(Activation('softmax'))\n    model.compile(loss=\"sparse_categorical_crossentropy\", metrics = [\"accuracy\"], optimizer = Adam(learning_rate=0.001))\n    model.summary()\n    history = model.fit(\n        flow,\n        epochs=100, \n        validation_data=(x_val, y_val),\n        callbacks= [\n            rlrop,\n            es,\n            mc,\n            tb,\n        ]\n    )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-08T05:32:02.617117Z","iopub.execute_input":"2024-11-08T05:32:02.617771Z","iopub.status.idle":"2024-11-08T07:38:51.364387Z","shell.execute_reply.started":"2024-11-08T05:32:02.617698Z","shell.execute_reply":"2024-11-08T07:38:51.359023Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"PASO 5: Plotear los valores de precisión, pérdidas y testear el modelo con los datos de prueba y exportar el resultado a csv.\n\nLos resultados del entrenamiento no aparecerán en este cuaderno ya que debido a que necesitaba optimizar mis tiempos con otras materias hice la mayor cantidad de pruebas, sobre todo las últimas, mediante el comando save & commit para poder dejarlo corriendo en el servidor. De todas formas mostraré en este mismo repositorio imágenes con los gráficos de convergencia del entrenamiento que guardo en este mismo paso. Hasta la fecha, la mejor puntuación que hice en submit fue de 0,3636 con la configuración actual.","metadata":{}},{"cell_type":"code","source":"plt.clf()\nplt.plot(history.history[\"loss\"], label=\"loss\")\nplt.plot(history.history[\"val_loss\"], label=\"val_loss\")\nplt.legend(loc=\"upper right\")\nplt.savefig(\"loss.png\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-08T07:38:51.372347Z","iopub.execute_input":"2024-11-08T07:38:51.373732Z","iopub.status.idle":"2024-11-08T07:38:51.904557Z","shell.execute_reply.started":"2024-11-08T07:38:51.373628Z","shell.execute_reply":"2024-11-08T07:38:51.903102Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.clf()\nplt.plot(history.history[\"accuracy\"], label=\"accuracy\")\nplt.plot(history.history[\"val_accuracy\"], label=\"val_accuracy\")\nplt.legend(loc=\"upper left\")\nplt.savefig(\"accuracy.png\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-08T07:38:51.907346Z","iopub.execute_input":"2024-11-08T07:38:51.908841Z","iopub.status.idle":"2024-11-08T07:38:52.213273Z","shell.execute_reply.started":"2024-11-08T07:38:51.908773Z","shell.execute_reply":"2024-11-08T07:38:52.211909Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\npredictions = model.predict(x_test).argmax(axis=1)\ndf = pd.DataFrame(predictions, columns=[\"Label\"])\ndf.index.name = \"Id\"\ndf.to_csv(\"submission.csv\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-08T07:38:52.215847Z","iopub.execute_input":"2024-11-08T07:38:52.216292Z","iopub.status.idle":"2024-11-08T07:38:52.465227Z","shell.execute_reply.started":"2024-11-08T07:38:52.216248Z","shell.execute_reply":"2024-11-08T07:38:52.463041Z"}},"outputs":[],"execution_count":null}]}