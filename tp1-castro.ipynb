{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"tpu1vmV38","dataSources":[{"sourceId":84511,"databundleVersionId":9468663,"sourceType":"competition"}],"dockerImageVersionId":30788,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"TP1: Implementación de una red MLP para clasificación de imágenes de CIFAR 100\n\nHecho por: Tomás Castro\n\nPASO 1: Abro todos los dataset de entrenamiento y prueba (ya se tantearon los datos brevemente en otra ocasión, así que en este cuaderno voy a ir directamente al entrenamiento)","metadata":{}},{"cell_type":"code","source":"# Cargo todos los datasets\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pickle\n%load_ext tensorboard\n\nINPUT_DIR = \"/kaggle/input/dl-itba-cifar-100-2024-q-1/\"\n\nx_train_ = np.load(INPUT_DIR+\"x_train.npy\")/255.\nx_test = np.load(INPUT_DIR+\"x_test.npy\")/255.\ny_train_coarse_ = np.load(INPUT_DIR+\"y_train_coarse.npy\")\ny_train_fine_ = np.load(INPUT_DIR+\"y_train_fine.npy\")\nwith open(INPUT_DIR+\"fine_label_names.pck\", \"rb\") as f:\n    labels_fine = pickle.load(f)\nwith open(INPUT_DIR+\"coarse_label_names.pck\", \"rb\") as f:\n    labels_coarse = pickle.load(f)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-11-08T14:58:40.317431Z","iopub.execute_input":"2024-11-08T14:58:40.318278Z","iopub.status.idle":"2024-11-08T14:58:43.21252Z","shell.execute_reply.started":"2024-11-08T14:58:40.318222Z","shell.execute_reply":"2024-11-08T14:58:43.211565Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"PASO 2: Activo los TPU","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\n# Detect TPU, return appropriate distribution strategy\ntry:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver() \n    print('Running on TPU ', tpu.master())\nexcept ValueError:\n    tpu = None\n\nif tpu:\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nelse:\n    strategy = tf.distribute.get_strategy() \n\nprint(\"REPLICAS: \", strategy.num_replicas_in_sync)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"\nPASO 3: Importo todas las funciones que voy a necesitar de keras y sklearn, además de que hago el augmentation sobre los datos de entrada.\n\nNOTA: Como las imágenes son de baja resolución, no puedo variar demasiado el contenido original de las imágenes","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Input, Flatten, Dense, BatchNormalization, Activation\nfrom tensorflow.keras.optimizers import SGD, Adam, RMSprop\nfrom tensorflow.keras.regularizers import l1, l2\nfrom tensorflow.keras.constraints import MaxNorm\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint, TensorBoard\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom sklearn.model_selection import train_test_split\n\n# Separación de datos en entrenamiento y validación\nx_train, x_val, y_train, y_val = train_test_split(x_train_, y_train_fine_, test_size=0.2, random_state=42, stratify=y_train_fine_)\n\n#Augmentation\ngen = ImageDataGenerator(\n        width_shift_range=0.05,\n        height_shift_range=0.05,\n        shear_range=0.05,\n        zoom_range=0.05,    \n        rotation_range = 10,\n        horizontal_flip=True)\nflow = gen.flow(x_train, y_train, batch_size=128)\n\n# Definición de callbacks\nrlrop = ReduceLROnPlateau(\n    monitor = \"val_accuracy\",\n    factor = 0.5,\n    patience = 3,\n    verbose = 1,\n    min_lr = 1e-5\n)\nmc = ModelCheckpoint(\n    \"best.weights.h5\",\n    monitor = \"val_accuracy\",\n    verbose = 1,\n    save_best_only = True,\n    save_weights_only = True,\n)\nes = EarlyStopping(\n    monitor = \"val_accuracy\",\n    patience = 10, \n    verbose = 1,\n    restore_best_weights = True,\n)\ntb = TensorBoard(\n    log_dir=\"logs\",\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-08T05:32:01.280137Z","iopub.execute_input":"2024-11-08T05:32:01.280672Z","iopub.status.idle":"2024-11-08T05:32:02.076081Z","shell.execute_reply.started":"2024-11-08T05:32:01.280619Z","shell.execute_reply":"2024-11-08T05:32:02.074401Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"PASO 4: Defino mi arquitectura. Tras diversas pruebas, se establecieron cuatro capas densas, iniciando con la primera capa del tamaño igual a la de entrada y luego se va reduciendo de a 1024 neuronas (32*32) hasta llegar a 100 en la última que es el tamaño de la salida. Se intercalaron capas de Batch Normalization antes de poner la activación, con muy buenos resultados y acelerando notoriamente el entrenamiento. Inicialmente la precisión de entrenamiento era mucho más grande que la de validación lo que era un indicio de sobreadaptación, pero tras haber agregado el augmentation las precisiones se igualaron. \n\nOtras cosas que se intentaron y no funcionaron:\n- Agregar capas de Dropout antes de las de Batch Normalization: la precisión empeoró\n- Agregar regularizadores L1 y L2 en las capas densas: el modelo ajustaba peor, con peor precisión\n- Agregar más capas: el entrenamiento era más lento y el modelo ajustaba peor","metadata":{}},{"cell_type":"code","source":"with strategy.scope():\n    model = Sequential()\n    model.add(Input(shape=(32,32,3)))\n    model.add(Flatten())\n    model.add(Dense(32*32*3, use_bias=False))\n    model.add(BatchNormalization())\n    model.add(Activation('relu'))\n    model.add(Dense(32*32*2, use_bias=False))\n    model.add(BatchNormalization())\n    model.add(Activation('relu'))\n    model.add(Dense(32*32, use_bias=False))\n    model.add(BatchNormalization())\n    model.add(Activation('relu'))\n    model.add(Dense(100, use_bias=False))\n    model.add(BatchNormalization())\n    model.add(Activation('softmax'))\n    model.compile(loss=\"sparse_categorical_crossentropy\", metrics = [\"accuracy\"], optimizer = Adam(learning_rate=0.001))\n    model.summary()\n    history = model.fit(\n        flow,\n        epochs=100, \n        validation_data=(x_val, y_val),\n        callbacks= [\n            rlrop,\n            es,\n            mc,\n            tb,\n        ]\n    )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-08T05:32:02.617117Z","iopub.execute_input":"2024-11-08T05:32:02.617771Z","iopub.status.idle":"2024-11-08T07:38:51.364387Z","shell.execute_reply.started":"2024-11-08T05:32:02.617698Z","shell.execute_reply":"2024-11-08T07:38:51.359023Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"PASO 5: Plotear los valores de precisión, pérdidas y testear el modelo con los datos de prueba y exportar el resultado a csv.\n\nLos resultados del entrenamiento no aparecerán en este cuaderno ya que debido a que necesitaba optimizar mis tiempos con otras materias hice la mayor cantidad de pruebas, sobre todo las últimas, mediante el comando save & commit para poder dejarlo corriendo en el servidor. De todas formas mostraré en este mismo repositorio imágenes con los gráficos de convergencia del entrenamiento que guardo en este mismo paso. Hasta la fecha, la mejor puntuación que hice en submit fue de 0,3636 con la configuración actual.","metadata":{}},{"cell_type":"code","source":"plt.plot(history.history[\"loss\"])\nplt.plot(history.history[\"val_loss\"])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-08T07:38:51.372347Z","iopub.execute_input":"2024-11-08T07:38:51.373732Z","iopub.status.idle":"2024-11-08T07:38:51.904557Z","shell.execute_reply.started":"2024-11-08T07:38:51.373628Z","shell.execute_reply":"2024-11-08T07:38:51.903102Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.plot(history.history[\"accuracy\"])\nplt.plot(history.history[\"val_accuracy\"])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-08T07:38:51.907346Z","iopub.execute_input":"2024-11-08T07:38:51.908841Z","iopub.status.idle":"2024-11-08T07:38:52.213273Z","shell.execute_reply.started":"2024-11-08T07:38:51.908773Z","shell.execute_reply":"2024-11-08T07:38:52.211909Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\npredictions = model.predict(x_test).argmax(axis=1)\ndf = pd.DataFrame(predictions, columns=[\"Label\"])\ndf.index.name = \"Id\"\ndf.to_csv(\"submission.csv\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-08T07:38:52.215847Z","iopub.execute_input":"2024-11-08T07:38:52.216292Z","iopub.status.idle":"2024-11-08T07:38:52.465227Z","shell.execute_reply.started":"2024-11-08T07:38:52.216248Z","shell.execute_reply":"2024-11-08T07:38:52.463041Z"}},"outputs":[],"execution_count":null}]}